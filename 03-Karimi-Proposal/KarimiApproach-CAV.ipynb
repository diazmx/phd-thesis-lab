{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karimi Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT CHANGE THIS CODE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from auxiliar_funcs import *\n",
    "\n",
    "import pmtools2 as pm\n",
    "import kmodes\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:  Index(['driving_task_type', 'control', 'monitoring', 'fallback', 'weather',\n",
      "       'visibility', 'traffic_congestion', 'environmental_weighted_average',\n",
      "       'driving_task_loa', 'vehicle_loa', 'region_loa', 'result'],\n",
      "      dtype='object')\n",
      "Lenght:  239580\n",
      "\n",
      "# User attr: 6\n",
      "# Rsrc attr: 3\n",
      "\n",
      "|U| = 6655\n",
      "|R| = 216\n",
      "\n",
      "|L+| = 118975 49.66%\n",
      "|L-| = 120605 50.34%\n",
      "Done!\n",
      " - k = 10\n",
      " - Train-Test size:  191664 ( 80.0 ) \t 47916 ( 20.0 )\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "url_file = '../00-Data/cav_policies.csv'\n",
    "cav_data = pd.read_csv(url_file)\n",
    "\n",
    "# Get a smaller sample: 15K positive and 15k negative.\n",
    "# cav_data = cav_data.groupby('result').sample(n=15000)\n",
    "print(\"Columns: \", cav_data.columns)\n",
    "print(\"Lenght: \", len(cav_data)); print()\n",
    "\n",
    "user_attr = ['control', 'monitoring', 'fallback', 'weather', 'visibility', \n",
    "        'traffic_congestion']\n",
    "#user_attr = ['control', 'monitoring', 'fallback']\n",
    "rsrc_attr = ['driving_task_loa', 'vehicle_loa', 'region_loa']\n",
    "cav_data = cav_data[user_attr + rsrc_attr + ['result']]\n",
    "\n",
    "# Change string values to numerical\n",
    "mapping = {'system': 10101, 'human': 10201, 'human and system': 10301} # Control\n",
    "cav_data.control = cav_data.control.replace(mapping)\n",
    "\n",
    "mapping = {'system': 20102, 'human': 20202} # monitoring\n",
    "cav_data.monitoring = cav_data.monitoring.replace(mapping)\n",
    "\n",
    "mapping = {'system': 30103, 'human': 30203} # fallbacj\n",
    "cav_data.fallback = cav_data.fallback.replace(mapping)\n",
    "\n",
    "mapping = {0: 40004, 1: 40104, 2: 40204, 3: 40304, 4: 40404, 5: 40504}\n",
    "cav_data.driving_task_loa = cav_data.driving_task_loa.replace(mapping)\n",
    "\n",
    "mapping = {0: 50005, 1: 50105, 2: 50205, 3: 50305, 4: 50405, 5: 50505}\n",
    "cav_data.vehicle_loa = cav_data.vehicle_loa.replace(mapping)\n",
    "\n",
    "mapping = {0: 60006, 1: 60106, 2: 60206, 3: 60306, 4: 60406, 5: 60506}\n",
    "cav_data.region_loa = cav_data.region_loa.replace(mapping)\n",
    "\n",
    "\n",
    "print(\"# User attr:\", len(user_attr))\n",
    "print(\"# Rsrc attr:\", len(rsrc_attr)); print()\n",
    "\n",
    "# Data statictics:\n",
    "n_users = len(cav_data[user_attr].drop_duplicates())\n",
    "n_rscrc = len(cav_data[rsrc_attr].drop_duplicates())\n",
    "print(\"|U| =\", n_users)\n",
    "print(\"|R| =\", n_rscrc); print()\n",
    "\n",
    "# Add user and resource id columns\n",
    "user_dict = get_user_res(cav_data, user_attr, True)\n",
    "rsrc_dict = get_user_res(cav_data, rsrc_attr, False)\n",
    "cav_data = add_col(cav_data, user_dict, user_attr, \"USRID\")\n",
    "cav_data = add_col(cav_data, rsrc_dict, rsrc_attr, \"RESID\")\n",
    "\n",
    "# Accepted and rejected requests\n",
    "cav_pos = cav_data[cav_data.result == 'approved']\n",
    "cav_neg = cav_data[cav_data.result == 'rejected']\n",
    "print(\"|L+| =\", len(cav_pos), \"{:.2f}%\".format((len(cav_pos) \n",
    "        / len(cav_data) ) * 100))\n",
    "print(\"|L-| =\", len(cav_neg), \"{:.2f}%\".format((len(cav_neg) \n",
    "        / len(cav_data) ) * 100))\n",
    "\n",
    "# Cross validation\n",
    "k = 10\n",
    "test_size = 0.2\n",
    "kfold = StratifiedShuffleSplit(n_splits=k, test_size=test_size, random_state=1)\n",
    "\n",
    "data_partition = kfold.split(cav_data, cav_data.result)\n",
    "data_curpus = [] # A list to storage the k folds\n",
    "\n",
    "for train_data, test_data in data_partition:\n",
    "    X_train, X_test = cav_data.iloc[train_data], cav_data.iloc[test_data]\n",
    "    data_curpus.append([X_train, X_test])\n",
    "\n",
    "print(\"Done!\")  \n",
    "print(\" - k =\", k)\n",
    "print(\" - Train-Test size: \", len(data_curpus[0][0]), \"(\", (1-test_size)*100, \") \\t\", len(data_curpus[0][1]), \"(\", test_size*100, \")\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "1. Continouos to Categorical Values\n",
    "2. Handle missing values - New value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train access request = 191664 80.00%\n",
      "# Train access request = 47916 80.00%\n",
      "Total = 239580\n",
      "\n",
      "TASK 1: Done!\n",
      "\n",
      "TASK 2: Done!\n",
      "\n",
      "TASK 3: Drop duplicates access requests\n",
      " -TRAIN DATA: Removing 0 positive access requests\n",
      " -TRAIN DATA: Removing 0 negative access requests\n",
      " -TEST DATA: Removing 0 positive access requests\n",
      " -TEST DATA: Removing 0 negative access requests\n",
      "Hecho!\n"
     ]
    }
   ],
   "source": [
    "id_kfold = 1\n",
    "\n",
    "cav_train, cav_test = data_curpus[id_kfold][0], data_curpus[id_kfold][1]\n",
    "print(\"# Train access request =\", len(cav_train), \"{:.2f}%\".format(\n",
    "    len(cav_train)/(len(cav_train)+len(cav_test))*100))\n",
    "print(\"# Train access request =\", len(cav_test), \"{:.2f}%\".format(\n",
    "    len(cav_train)/(len(cav_train)+len(cav_test))*100))\n",
    "print(\"Total =\", len(cav_train)+len(cav_test)); print()\n",
    "\n",
    "#### **** SELECT FUNCTIONAL ATTRIBUTES **** ####\n",
    "cav_train = cav_train[user_attr + rsrc_attr + ['USRID', 'RESID', 'result']]\n",
    "cav_test = cav_test[user_attr + rsrc_attr + ['USRID', 'RESID', 'result']]\n",
    "\n",
    "##### ***** Task 1: Null and uknwokn values ***** #####\n",
    "print(\"TASK 1: Done!\"); print() # NA\n",
    "\n",
    "\n",
    "##### ***** TASK 2: convert continuous values to categorical values ***** #####\n",
    "print(\"TASK 2: Done!\"); print() # NA \n",
    "\n",
    "##### ***** TASK 3: Drop duplicates access requests ***** #####\n",
    "print(\"TASK 3: Drop duplicates access requests\")\n",
    "\n",
    "positive_cav_train = cav_train[cav_train.result=='approved']\n",
    "positive_cav_test = cav_test[cav_test.result=='approved']\n",
    "negative_cav_train = cav_train[cav_train.result=='rejected']\n",
    "negative_cav_test = cav_test[cav_test.result=='rejected']\n",
    "\n",
    "\n",
    "print(\" -TRAIN DATA: Removing\", \n",
    "    len(positive_cav_train.drop_duplicates()) - \n",
    "    len(positive_cav_train), \"positive access requests\")\n",
    "print(\" -TRAIN DATA: Removing\", \n",
    "    len(negative_cav_train.drop_duplicates()) - \n",
    "    len(negative_cav_train), \"negative access requests\")\n",
    "print(\" -TEST DATA: Removing\", \n",
    "    len(positive_cav_test.drop_duplicates()) - \n",
    "    len(positive_cav_test), \"positive access requests\")\n",
    "print(\" -TEST DATA: Removing\", \n",
    "    len(negative_cav_test.drop_duplicates()) - \n",
    "    len(negative_cav_test), \"negative access requests\")\n",
    "\n",
    "# Filter resources\n",
    "#bolean_series = negative_cav_train.RESID.isin(top_list)\n",
    "#negative_cav_train = negative_cav_train[bolean_series]\n",
    "print(\"Hecho!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of Learning Algorithm\n",
    "1. K-modes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "###Select the number of clusters###\n",
    "num_clusters = 20\n",
    "\n",
    "#DO NOT CHANGE THIS CODE\n",
    "# seed = 29\n",
    "\n",
    "#Compute centroids and labels\n",
    "# num_init = 5\n",
    "centroids = []\n",
    "kmodes_huang = kmodes.KModes(n_clusters=num_clusters, init='Huang', verbose=0)\n",
    "cluster_labels = kmodes_huang.fit_predict(positive_cav_train)\n",
    "centroids = kmodes_huang.cluster_centroids_\n",
    "\n",
    "print('Ready!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/envs/tesis_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "positive_cav_train[\"cls\"] = cluster_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "1. Number of clusters (Silhouette Method)\n",
    "2. Cluster initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Rules Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(value, attribute, dataplace):\n",
    "    \"\"\"\n",
    "    Calculate the frequency of the value in the dataplace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    value : int\n",
    "        Value to compute its frequency.\n",
    "\n",
    "    attribute : string\n",
    "        Name of the attribute.\n",
    "\n",
    "    dataplace : DataFrame pandas\n",
    "        Data to search.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float [0-1]\n",
    "        Returns the value of frequency of the value in the data.\n",
    "    \"\"\"\n",
    "    value_freq = dataplace[dataplace[attribute] == value].drop_duplicates()\n",
    "    return len(value_freq) / len(dataplace)\n",
    "\n",
    "def freq_rels(attrA, attrB, dataplace):\n",
    "    \"\"\"\n",
    "    Compute the frequency of the attribute relation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attrA : string\n",
    "        Name of the attribute A to compare.\n",
    "    attrB : string\n",
    "        Name of the attribute B to compare.\n",
    "    dataplace : DataFrame pandas\n",
    "        Data to search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float [0-1]\n",
    "        Returns the value of frequency of the range of values in the data.\n",
    "    \"\"\"\n",
    "    # Get the range of values of attribute A.\n",
    "    range_val_A = set(dataplace[attrA].values.tolist())\n",
    "\n",
    "    # Get the range of values of attribute B.\n",
    "    range_val_B = set(dataplace[attrB].values.tolist())\n",
    "\n",
    "    # Check if the len\n",
    "    if len(range_val_A) == len(range_val_B):\n",
    "        # Compute the intersection\n",
    "        inter_A_B = range_val_A.intersection(range_val_B)\n",
    "        if len(inter_A_B) == len(range_val_A):\n",
    "            boolean_series = dataplace[attrA].isin(inter_A_B)\n",
    "            frac_log = dataplace[boolean_series]\n",
    "            return len(frac_log) / len(dataplace) # Return the fraction\n",
    "        return 0\n",
    "    else:\n",
    "        return 0    \n",
    "\n",
    "def extract_attributes_filters(C_i, A, L, posThr, negThr):\n",
    "    \"\"\"\n",
    "    Effective attribute extraction algorithm. Generate a rule for each cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    C_i : DataFrame pandas\n",
    "        Access request in the Cluster i.\n",
    "\n",
    "    A : List\n",
    "        List of attributes.\n",
    "\n",
    "    V : List\n",
    "        Values of attributes.\n",
    "\n",
    "    L : DataFrame\n",
    "        Complete Access Log.\n",
    "\n",
    "    PosThr : float\n",
    "        Positive Threshold to the effective positive attribute.\n",
    "\n",
    "    NegThr : float\n",
    "        Negative Threshold to the effective negative attribute.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Returns the rule with the effective attributes for the cluster i.\n",
    "    \"\"\"\n",
    "    filter_to_ret = [] # Rule\n",
    "    for a in A:        \n",
    "        a_values = C_i[a].drop_duplicates().tolist()        \n",
    "        for v in a_values:\n",
    "            if freq(v, a, C_i) - freq(v, a, L) > posThr:\n",
    "                if not [a, v] in filter_to_ret:\n",
    "                    filter_to_ret.append([a, v])\n",
    "            if freq(v, a, L) - freq(v, a, C_i) > negThr:\n",
    "                if not [a, -1*v] in filter_to_ret:\n",
    "                    filter_to_ret.append([a, v*-1])\n",
    "    return filter_to_ret\n",
    "\n",
    "def extract_relations(C_i, A, L, posThr, negThr):\n",
    "    \"\"\"\n",
    "    Extract the effective relation. For each cluster.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    C_i : DataFrame pandas\n",
    "        Access request in the Cluster i.\n",
    "\n",
    "    A : List\n",
    "        List of attributes.\n",
    "\n",
    "    L : DataFrame\n",
    "        Complete Access Log.\n",
    "\n",
    "    posThr : float\n",
    "        Positive Threshold to the effective positive relation.\n",
    "\n",
    "    negThr : float\n",
    "        Negative Threshold to the effective negative relation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Returns the rule with the effective relation for the cluster i.\n",
    "    \"\"\"\n",
    "    relation_to_ret = []\n",
    "    for a in A:\n",
    "        for b in A:\n",
    "            if a != b:\n",
    "                if freq_rels(a, b, C_i) - freq_rels(a, b, L) > posThr:\n",
    "                    if not [a, b] in relation_to_ret:\n",
    "                        relation_to_ret.append([a, b])\n",
    "                if freq_rels(a, b, L) - freq_rels(a, b, C_i) > negThr:\n",
    "                    if not [a, '!'+b] in relation_to_ret:                        \n",
    "                        relation_to_ret.append([a, '!'+b])\n",
    "                    print()\n",
    "\n",
    "def rule_inference(data_, pos_attr_thr, \n",
    "    neg_attr_thr, pos_rel_thr, neg_rel_thr):\n",
    "    rule_list = [] # All rules\n",
    "    n_cluster = len(data_[\"cls\"].drop_duplicates()) # N clusters\n",
    "    attrs = data_.columns[:-1] # Name of the columns\n",
    "\n",
    "    for C_i in range(n_cluster):\n",
    "        #print(C_i)\n",
    "        rule_i = []\n",
    "        data_cluster = data_[data_[\"cls\"] == C_i]\n",
    "        \n",
    "        # Effective attributes\n",
    "        attr_filters = extract_attributes_filters(data_cluster, attrs, data_, \n",
    "            pos_attr_thr, neg_attr_thr)    \n",
    "        rule_i.append(attr_filters)        \n",
    "\n",
    "        # Relations\n",
    "        attr_relation = extract_relations(data_cluster, attrs, data_, \n",
    "            pos_rel_thr, neg_rel_thr)\n",
    "        rule_i.append(attr_relation)\n",
    "        #print(rule_i)\n",
    "\n",
    "        rule_list.append([C_i, rule_i])    \n",
    "\n",
    "    return rule_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['monitoring', 'fallback', 'weather', 'visibility', 'traffic_congestion',\n",
       "       'driving_task_loa', 'vehicle_loa', 'region_loa', 'cls'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = positive_cav_train[['monitoring', 'fallback', 'weather', 'visibility', 'traffic_congestion',\n",
    "       'driving_task_loa', 'vehicle_loa', 'region_loa', 'cls']]\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  [[['driving_task_loa', 40104],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['region_loa', 60106]],\n",
       "   None]],\n",
       " [1, [[['driving_task_loa', 40004], ['vehicle_loa', 50105]], None]],\n",
       " [2,\n",
       "  [[['driving_task_loa', 40104],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['region_loa', 60406]],\n",
       "   None]],\n",
       " [3,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50005],\n",
       "    ['region_loa', 60106]],\n",
       "   None]],\n",
       " [4,\n",
       "  [[['driving_task_loa', 40104],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['region_loa', 60506]],\n",
       "   None]],\n",
       " [5,\n",
       "  [[['driving_task_loa', 40104],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['vehicle_loa', 50205],\n",
       "    ['region_loa', 60206]],\n",
       "   None]],\n",
       " [6,\n",
       "  [[['driving_task_loa', 40104],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['vehicle_loa', 50405],\n",
       "    ['region_loa', 60306]],\n",
       "   None]],\n",
       " [7,\n",
       "  [[['driving_task_loa', 40204],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['region_loa', 60206]],\n",
       "   None]],\n",
       " [8,\n",
       "  [[['driving_task_loa', 40104],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['vehicle_loa', 50505],\n",
       "    ['region_loa', 60206]],\n",
       "   None]],\n",
       " [9,\n",
       "  [[['monitoring', 20102],\n",
       "    ['monitoring', -20202],\n",
       "    ['driving_task_loa', 40304],\n",
       "    ['driving_task_loa', -40004]],\n",
       "   None]],\n",
       " [10,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50505],\n",
       "    ['region_loa', 60206]],\n",
       "   None]],\n",
       " [11,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50305],\n",
       "    ['region_loa', 60306]],\n",
       "   None]],\n",
       " [12,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50205],\n",
       "    ['region_loa', 60006]],\n",
       "   None]],\n",
       " [13,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50005],\n",
       "    ['region_loa', 60506]],\n",
       "   None]],\n",
       " [14,\n",
       "  [[['driving_task_loa', 40204],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['region_loa', 60406]],\n",
       "   None]],\n",
       " [15,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50205],\n",
       "    ['region_loa', 60306]],\n",
       "   None]],\n",
       " [16,\n",
       "  [[['monitoring', 20102],\n",
       "    ['monitoring', -20202],\n",
       "    ['fallback', 30103],\n",
       "    ['fallback', -30203],\n",
       "    ['driving_task_loa', 40404],\n",
       "    ['vehicle_loa', 50505],\n",
       "    ['region_loa', 60506]],\n",
       "   None]],\n",
       " [17,\n",
       "  [[['weather', 0],\n",
       "    ['traffic_congestion', 9],\n",
       "    ['driving_task_loa', 40104],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['region_loa', 60506]],\n",
       "   None]],\n",
       " [18,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50305],\n",
       "    ['region_loa', 60406]],\n",
       "   None]],\n",
       " [19,\n",
       "  [[['weather', 10],\n",
       "    ['visibility', 2],\n",
       "    ['traffic_congestion', 10],\n",
       "    ['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50005]],\n",
       "   None]]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_attr_thr = 0.3\n",
    "neg_attr_thr = 0.3\n",
    "pos_rel_thr = 0.2\n",
    "neg_rel_thr = 0.2\n",
    "test = rule_inference(df_test, pos_attr_thr, neg_attr_thr, pos_rel_thr, neg_attr_thr)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['driving_task_loa', 40104],\n",
       "  ['driving_task_loa', -40004],\n",
       "  ['region_loa', 60106]],\n",
       " [['driving_task_loa', 40004], ['vehicle_loa', 50105]],\n",
       " [['driving_task_loa', 40104],\n",
       "  ['driving_task_loa', -40004],\n",
       "  ['region_loa', 60406]],\n",
       " [['driving_task_loa', 40004], ['vehicle_loa', 50005], ['region_loa', 60106]],\n",
       " [['driving_task_loa', 40104],\n",
       "  ['driving_task_loa', -40004],\n",
       "  ['region_loa', 60506]],\n",
       " [['driving_task_loa', 40104],\n",
       "  ['driving_task_loa', -40004],\n",
       "  ['vehicle_loa', 50205],\n",
       "  ['region_loa', 60206]],\n",
       " [['driving_task_loa', 40104],\n",
       "  ['driving_task_loa', -40004],\n",
       "  ['vehicle_loa', 50405],\n",
       "  ['region_loa', 60306]],\n",
       " [['driving_task_loa', 40204],\n",
       "  ['driving_task_loa', -40004],\n",
       "  ['region_loa', 60206]],\n",
       " [['driving_task_loa', 40104],\n",
       "  ['driving_task_loa', -40004],\n",
       "  ['vehicle_loa', 50505],\n",
       "  ['region_loa', 60206]],\n",
       " [['monitoring', 20102],\n",
       "  ['monitoring', -20202],\n",
       "  ['driving_task_loa', 40304],\n",
       "  ['driving_task_loa', -40004]],\n",
       " [['driving_task_loa', 40004], ['vehicle_loa', 50505], ['region_loa', 60206]],\n",
       " [['driving_task_loa', 40004], ['vehicle_loa', 50305], ['region_loa', 60306]],\n",
       " [['driving_task_loa', 40004], ['vehicle_loa', 50205], ['region_loa', 60006]],\n",
       " [['driving_task_loa', 40004], ['vehicle_loa', 50005], ['region_loa', 60506]],\n",
       " [['driving_task_loa', 40204],\n",
       "  ['driving_task_loa', -40004],\n",
       "  ['region_loa', 60406]],\n",
       " [['driving_task_loa', 40004], ['vehicle_loa', 50205], ['region_loa', 60306]],\n",
       " [['monitoring', 20102],\n",
       "  ['monitoring', -20202],\n",
       "  ['fallback', 30103],\n",
       "  ['fallback', -30203],\n",
       "  ['driving_task_loa', 40404],\n",
       "  ['vehicle_loa', 50505],\n",
       "  ['region_loa', 60506]],\n",
       " [['weather', 0],\n",
       "  ['traffic_congestion', 9],\n",
       "  ['driving_task_loa', 40104],\n",
       "  ['driving_task_loa', -40004],\n",
       "  ['region_loa', 60506]],\n",
       " [['driving_task_loa', 40004], ['vehicle_loa', 50305], ['region_loa', 60406]],\n",
       " [['weather', 10],\n",
       "  ['visibility', 2],\n",
       "  ['traffic_congestion', 10],\n",
       "  ['driving_task_loa', 40004],\n",
       "  ['vehicle_loa', 50005]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_rules = []\n",
    "for rule in test:\n",
    "    only_rules.append(rule[1][0])\n",
    "only_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute how many rules has a lenght equal to 1.\n",
    "for idx, rule in enumerate(only_rules):\n",
    "    if len(rule) == 1:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg  = []\n",
    "for i,row in positive_cav_train.iterrows():\n",
    "    \n",
    "    # Evaluaci贸n\n",
    "    denies_count = 0\n",
    "    #print(row)\n",
    "    \n",
    "    for rule in only_rules:                                      \n",
    "        # En esta parte se evalua la regla completa\n",
    "        res = True\n",
    "        #print(rule)\n",
    "        #for idx_r, attr_val in enumerate(rule):\n",
    "        for attr_val in rule:            \n",
    "            if attr_val[1] < 0:\n",
    "                if row[attr_val[0]] == attr_val[1]*-1:\n",
    "                    res = False\n",
    "                    #print(res)\n",
    "                    break\n",
    "            else:\n",
    "                if row[attr_val[0]] != attr_val[1]:\n",
    "                    res = False\n",
    "                    break\n",
    "        if res == False:\n",
    "            denies_count += 1\n",
    "    \n",
    "    if denies_count == len(only_rules):\n",
    "        false_neg.append(row)\n",
    "        #print(\"FP-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa FN: 45.47\n",
      "FN:  43275  de  95180\n"
     ]
    }
   ],
   "source": [
    "FN = len(false_neg)\n",
    "print(\"Tasa FN: {:.2f}\".format((FN/ len(positive_cav_train))*100))\n",
    "print(\"FN: \", FN, \" de \", len(positive_cav_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos  = []\n",
    "for i,row in negative_cav_train.iterrows():\n",
    "    # Evaluaci贸n\n",
    "    denies_count = 0\n",
    "    temp_rules_n = 0\n",
    "    #print(row)\n",
    "    for rule in only_rules:                                      \n",
    "        # En esta parte se evalua la regla completa\n",
    "        res = True\n",
    "        #print(rule)                     \n",
    "        for attr_val in rule:\n",
    "            if attr_val[1] < 0:\n",
    "                #print(attr_val)\n",
    "                if row[attr_val[0]] == attr_val[1]*-1:\n",
    "                    res = False\n",
    "                    break\n",
    "            else:\n",
    "                if row[attr_val[0]] != attr_val[1]:\n",
    "                    res = False\n",
    "                    break\n",
    "        if res == False:\n",
    "            denies_count += 1                                \n",
    "    #print(\"XXX-\", denies_count, temp_rules_n, res)\n",
    "    #print(\"R\", denies_count, len(only_rules))\n",
    "    if denies_count < len(only_rules):\n",
    "        false_pos.append(row)\n",
    "        #print(\"FP-2\")    \n",
    "    #else:\n",
    "        #print(\"ENtra PAPA\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa FP: 37.38\n",
      "FN:  36061  de  96484\n"
     ]
    }
   ],
   "source": [
    "FP = len(false_pos)\n",
    "print(\"Tasa FP: {:.2f}\".format((FP/ len(negative_cav_train))*100))\n",
    "print(\"FN: \", FP, \" de \", len(negative_cav_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FN: 43275  - 45.47\n",
      "FP: 36061  - 37.38\n",
      "Precision: 0.5900575222244958\n",
      "Recall: 0.545335154444211\n",
      "F-score 0.5668155460670722\n",
      "# Rules: 20\n",
      "WSC: 71\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TP = len(positive_cav_train) - FN\n",
    "#TP = 50 - FN\n",
    "TN = len(negative_cav_train) - FP\n",
    "#TN = 50 - FP\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "fscore = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(\"FN:\", FN, \" - {:.2f}\".format((FN/len(positive_cav_train))*100))\n",
    "#print(\"FN:\", FN, \" - {:.2f}\".format((FN/50)*100))\n",
    "print(\"FP:\", FP, \" - {:.2f}\".format((FP/len(negative_cav_train))*100))\n",
    "#print(\"FP:\", FP, \" - {:.2f}\".format((FP/50)*100))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score\", fscore)\n",
    "\n",
    "def compute_wsc(policy):\n",
    "    return sum([len(rule) for rule in policy])\n",
    "\n",
    "print(\"# Rules:\", len(only_rules))\n",
    "print(\"WSC:\", compute_wsc(only_rules))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Enhancement\n",
    "1. Rule Pruning\n",
    "2. Policy Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "false_neg = pd.DataFrame(false_neg)\n",
    "\n",
    "###Select the number of clusters###\n",
    "num_clusters = 10\n",
    "\n",
    "#DO NOT CHANGE THIS CODE\n",
    "# seed = 29\n",
    "\n",
    "#Compute centroids and labels\n",
    "# num_init = 5\n",
    "centroids = []\n",
    "kmodes_huang = kmodes.KModes(n_clusters=num_clusters, init='Huang', verbose=0)\n",
    "cluster_labels = kmodes_huang.fit_predict(false_neg)\n",
    "centroids = kmodes_huang.cluster_centroids_\n",
    "\n",
    "print('Ready!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg[\"cls\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_2 = false_neg[['monitoring', 'fallback', 'weather', 'visibility', \n",
    "    'traffic_congestion', 'driving_task_loa', 'vehicle_loa', 'region_loa', 'cls']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  [[['driving_task_loa', 40204],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['region_loa', 60506]],\n",
       "   None]],\n",
       " [1,\n",
       "  [[['driving_task_loa', 40204],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['region_loa', 60306]],\n",
       "   None]],\n",
       " [2, [[['driving_task_loa', 40004], ['region_loa', -60306]], None]],\n",
       " [3,\n",
       "  [[['driving_task_loa', 40104],\n",
       "    ['driving_task_loa', -40004],\n",
       "    ['region_loa', 60306]],\n",
       "   None]],\n",
       " [4,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50005],\n",
       "    ['vehicle_loa', -50405],\n",
       "    ['region_loa', -60506]],\n",
       "   None]],\n",
       " [5, [[['driving_task_loa', 40004], ['vehicle_loa', 50505]], None]],\n",
       " [6, [[['driving_task_loa', 40004], ['vehicle_loa', 50405]], None]],\n",
       " [7,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['vehicle_loa', 50405],\n",
       "    ['vehicle_loa', -50505],\n",
       "    ['region_loa', 60406],\n",
       "    ['region_loa', -60306]],\n",
       "   None]],\n",
       " [8,\n",
       "  [[['visibility', 2], ['traffic_congestion', 0], ['driving_task_loa', 40004]],\n",
       "   None]],\n",
       " [9,\n",
       "  [[['driving_task_loa', 40004],\n",
       "    ['region_loa', 60106],\n",
       "    ['region_loa', -60306]],\n",
       "   None]]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_attr_thr = 0.3\n",
    "neg_attr_thr = 0.2\n",
    "pos_rel_thr = 0.3\n",
    "neg_rel_thr = 0.2\n",
    "test = rule_inference(df_test_2, pos_attr_thr, neg_attr_thr, pos_rel_thr, neg_attr_thr)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only_rules = []\n",
    "for rule in test:\n",
    "    only_rules.append(rule[1][0])\n",
    "len(only_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute how many rules has a lenght equal to 1.\n",
    "for idx, rule in enumerate(only_rules):\n",
    "    if len(rule) == 1:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa FN: 6.42\n",
      "FN:  6110  de  95180\n"
     ]
    }
   ],
   "source": [
    "false_neg  = []\n",
    "for i,row in positive_cav_train.iterrows():\n",
    "    \n",
    "    # Evaluaci贸n\n",
    "    denies_count = 0    \n",
    "    for rule in only_rules:                                      \n",
    "        # En esta parte se evalua la regla completa\n",
    "        res = True\n",
    "        \n",
    "        #for idx_r, attr_val in enumerate(rule):\n",
    "        for attr_val in rule:            \n",
    "            if attr_val[1] < 0:\n",
    "                if row[attr_val[0]] == attr_val[1]*-1:\n",
    "                    res = False\n",
    "                    break\n",
    "            else:\n",
    "                if row[attr_val[0]] != attr_val[1]:\n",
    "                    res = False\n",
    "                    break\n",
    "        if res == False:\n",
    "            denies_count += 1\n",
    "    \n",
    "    if denies_count == len(only_rules):\n",
    "        false_neg.append(row)\n",
    "        #print(\"FP-2\")\n",
    "\n",
    "FN = len(false_neg)\n",
    "print(\"Tasa FN: {:.2f}\".format((FN/ len(positive_cav_train))*100))\n",
    "print(\"FN: \", FN, \" de \", len(positive_cav_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa FP: 42.82\n",
      "FN:  41313  de  96484\n"
     ]
    }
   ],
   "source": [
    "false_pos  = []\n",
    "for i,row in negative_cav_train.iterrows():\n",
    "    # Evaluaci贸n\n",
    "    denies_count = 0\n",
    "    temp_rules_n = 0\n",
    "    for rule in only_rules:                                      \n",
    "        # En esta parte se evalua la regla completa\n",
    "        res = True                        \n",
    "        for attr_val in rule:\n",
    "            if attr_val[1] < 0:\n",
    "                if row[attr_val[0]] == attr_val[1]*-1:\n",
    "                    res = False\n",
    "                    break\n",
    "            else:\n",
    "                if row[attr_val[0]] != attr_val[1]:\n",
    "                    res = False\n",
    "                    break\n",
    "        if res == False:\n",
    "            denies_count += 1                                \n",
    "    #print(\"XXX-\", denies_count, temp_rules_n, res)\n",
    "    if denies_count < len(only_rules):\n",
    "        false_pos.append(row)\n",
    "        #print(\"FP-2\")    \n",
    "    #else:\n",
    "    #    print(\"ENtra PAPA\")\n",
    "FP = len(false_pos)\n",
    "print(\"Tasa FP: {:.2f}\".format((FP/ len(negative_cav_train))*100))\n",
    "print(\"FN: \", FP, \" de \", len(negative_cav_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FN: 6110  - 6.42\n",
      "FP: 41313  - 42.82\n",
      "Precision: 0.6831412070592024\n",
      "Recall: 0.9358058415633537\n",
      "F-score 0.7897571853539809\n",
      "# Rules: 30\n",
      "WSC: 101\n"
     ]
    }
   ],
   "source": [
    "TP = len(positive_cav_train) - FN\n",
    "#TP = 50 - FN\n",
    "TN = len(negative_cav_train) - FP\n",
    "#TN = 50 - FP\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "fscore = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(\"FN:\", FN, \" - {:.2f}\".format((FN/len(positive_cav_train))*100))\n",
    "#print(\"FN:\", FN, \" - {:.2f}\".format((FN/50)*100))\n",
    "print(\"FP:\", FP, \" - {:.2f}\".format((FP/len(negative_cav_train))*100))\n",
    "#print(\"FP:\", FP, \" - {:.2f}\".format((FP/50)*100))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-score\", fscore)\n",
    "\n",
    "def compute_wsc(policy):\n",
    "    return sum([len(rule) for rule in policy])\n",
    "\n",
    "print(\"# Rules:\", len(only_rules))\n",
    "print(\"WSC:\", compute_wsc(only_rules))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "deec884ae07beaa7e970d39d84b9343805b80aeba4e343861fb4097b62dc7004"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
